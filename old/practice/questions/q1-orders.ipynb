{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7decf01-4ee7-40f1-8c56-6d9867ff3dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Task 1 — Deduplication + Latest State + SCD Thinking\n",
    "**Scenario:** You receive daily order status updates.\n",
    "\n",
    "**Schema:**\n",
    "```\n",
    "order_id (string)\n",
    "customer_id (string)\n",
    "status (string)\n",
    "amount (double)\n",
    "updated_at (timestamp)\n",
    "```\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "- Remove duplicate records.\n",
    "- Keep only the latest record per order_id based on updated_at.\n",
    "- Create a new column:\n",
    "is_high_value = 1 if amount > 1000 else 0\n",
    "- Return final cleaned DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20089eaa-0407-44ba-832e-9ffcf36012c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = spark.read.table('pyspark.sampledata.q1orders')\n",
    "df = df.dropDuplicates([\"order_id\", \"updated_at\"])\n",
    "\n",
    "window_spec = Window.partitionBy('order_id').orderBy(\n",
    "    F.col('updated_at').desc()\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "\n",
    "\n",
    "df = df.filter(F.col(\"rn\")==1).drop(\"rn\")\n",
    "\n",
    "\n",
    "df = df.withColumn(\"is_high_value\", F.when(F.col(\"amount\")>1000, 1).otherwise(0))\n",
    "\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1635099-80b5-496a-9d54-eff2f49f30ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('pyspark.sampledata.q1orders')\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622c33e5-4dcc-498d-8c93-762bdf790d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9ec1ef-7a4f-4a4a-89a5-fee733b53af9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4fd811-bddb-4444-93b2-d53c49575752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('order_id').orderBy(\n",
    "    F.col('updated_at').desc()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba90a9b-adf6-45fc-bd0c-83da3ed669b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"rn\", F.row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e59d5d4-e842-4da2-9e24-008da399112d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883b6d74-0836-4a77-9672-587391ce1071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.filter(F.col(\"rn\")==1).drop(\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b249b58b-ef42-4441-8fca-a1bfade4e696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"is_high_value\", F.when(F.col(\"amount\")>1000, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b692263-5596-48ef-b4d9-8266442a8e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8192fdc-4032-4249-a21f-8e0d5d687ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "497e34e5-f7e6-42e8-b74a-8d8616923236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Follow-up Questions:\n",
    "\n",
    "How would you optimise this for 500M rows?\n",
    "\n",
    "What causes shuffle here?\n",
    "\n",
    "How would you implement SCD Type 2 instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c99f9e8-6f3e-4580-819d-f0d30faeb082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#another solution \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.table(\"pyspark.sampledata.q1orders\").dropDuplicates()\n",
    "\n",
    "# get latest timestamp per order\n",
    "latest_ts = df.groupBy(\"order_id\") \\\n",
    "              .agg(F.max(\"updated_at\").alias(\"updated_at\"))\n",
    "\n",
    "# join back to original to get full record\n",
    "df_latest = df.join(latest_ts, [\"order_id\", \"updated_at\"], \"inner\")\n",
    "\n",
    "# derive column\n",
    "df_final = df_latest.withColumn(\n",
    "    \"is_high_value\",\n",
    "    F.when(F.col(\"amount\") > 1000, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "display(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645e7c6f-ab6b-43c4-bfff-67da76d8d138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = df.dropDuplicates()\n",
    "max_df = df.groupBy(\"order_id\").agg(\n",
    "    F.max(\"updated_at\")\n",
    "    .alias(\"max\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb463aa2-5acc-485c-a59d-6e22a1f271fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = df.join(max_df, \n",
    "        ((df.order_id == max_df.order_id) &\n",
    "        (df.updated_at == max_df.max)) ,\n",
    "        \"inner\"\n",
    "        )\\\n",
    "        .withColumn(\"is_high_value\",F.when(F.col(\"amount\")>1000,1).otherwise(0))\\\n",
    "        .drop(max_df.order_id) \\\n",
    "        .drop(max_df.max)\\\n",
    "        .orderBy(\"order_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e918f52-27b6-4372-b044-e4fdf44c8556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.write.mode(\"overwrite\").saveAsTable(\"pyspark.sampledata.out_q1orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb93a0b7-f1d6-4626-b27f-6d88bb04f90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.withColumn(\"double_amount\",F.expr(\"\"\"\n",
    "                        case\n",
    "                        when amount > 1000 then amount * 2\n",
    "                        else amount\n",
    "                        end\n",
    "                    \"\"\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c18b88-a27f-45ef-9e68-741db1718de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee407b00-ee12-4b00-9ec8-ee18f4b245b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How would you optimise for 500M rows?”\n",
    "\n",
    "\n",
    "**For 500M rows, I’d optimise in multiple ways:**\n",
    "\n",
    "- Reduce data early by selecting only required columns and deduplicating on business keys\n",
    "\n",
    "- Avoid window functions and use aggregation + join or max_by() to reduce shuffle\n",
    "\n",
    "- Repartition by order_id and tune shuffle partitions\n",
    "\n",
    "- Handle skew if certain keys are heavy\n",
    "\n",
    "- Store results in Delta and use ZORDER for faster reads\n",
    "\n",
    "- For production, I’d make this incremental using MERGE instead of recomputing everything daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04dbd031-b372-4f40-9eff-2f0772155d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = spark.read.table(\"pyspark.sampledata.q1orders\")\\\n",
    "    .dropDuplicates()\\\n",
    "    .withColumn(\"is_high_value\",F.when(F.col(\"amount\")>1000,1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b1e700-1970-4be6-98ea-a060002a60ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target = spark.read.table(\"pyspark.sampledata.out_q1orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc5f00e9-e7cd-4780-895a-149ba1008822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    target.alias(\"t\")\n",
    "    .merge(\n",
    "        source.alias(\"s\"),\n",
    "        \"t.order_id = s.order_id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"s.updated_at > t.updated_at\",\n",
    "        set={\n",
    "            \"customer_id\": \"s.customer_id\",\n",
    "            \"status\": \"s.status\",\n",
    "            \"amount\": \"s.amount\",\n",
    "            \"updated_at\": \"s.updated_at\",\n",
    "            \"is_high_value\": \"s.is_high_value\"\n",
    "        }\n",
    "    )\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"order_id\": \"s.order_id\",\n",
    "            \"customer_id\": \"s.customer_id\",\n",
    "            \"status\": \"s.status\",\n",
    "            \"amount\": \"s.amount\",\n",
    "            \"updated_at\": \"s.updated_at\",\n",
    "            \"is_high_value\": \"s.is_high_value\"\n",
    "        }\n",
    "    )\n",
    "    .execute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1efed408-1f40-49d7-9d12-084d6bd8e4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# How would you implement SCD Type 2 instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98ff3fb-6e3c-4fd2-afc8-66aa8f73910d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "#Step 1 — Prepare incoming data\n",
    "updates_df = (\n",
    "    spark.read.table(\"pyspark.sampledata.q1orders\")\n",
    "    .dropDuplicates([\"order_id\", \"updated_at\"])\n",
    "    .withColumn(\"is_high_value\", F.when(F.col(\"amount\") > 1000, 1).otherwise(0))\n",
    "    .withColumn(\"effective_start_date\", F.col(\"updated_at\"))\n",
    "    .withColumn(\"effective_end_date\", F.lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"is_current\", F.lit(True))\n",
    ")\n",
    "\n",
    "#Step 2 — Load target table\n",
    "target = DeltaTable.forName(spark, \"orders_scd2\")\n",
    "\n",
    "#Step 3 — Define change condition\n",
    "change_condition = \"\"\"\n",
    "t.customer_id <> s.customer_id OR\n",
    "t.status <> s.status OR\n",
    "t.amount <> s.amount OR\n",
    "t.is_high_value <> s.is_high_value\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Step 4 — MERGE for SCD Type 2\n",
    "(\n",
    "    target.alias(\"t\")\n",
    "    .merge(\n",
    "        updates_df.alias(\"s\"),\n",
    "        \"t.order_id = s.order_id AND t.is_current = true\"\n",
    "    )\n",
    "    \n",
    "    # 1️⃣ expire old record\n",
    "    .whenMatchedUpdate(\n",
    "        condition = f\"{change_condition} AND s.updated_at > t.effective_start_date\",\n",
    "        set = {\n",
    "            \"effective_end_date\": \"s.updated_at\",\n",
    "            \"is_current\": \"false\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 2️⃣ insert new record\n",
    "    .whenNotMatchedInsert(values={\n",
    "        \"order_id\": \"s.order_id\",\n",
    "        \"customer_id\": \"s.customer_id\",\n",
    "        \"status\": \"s.status\",\n",
    "        \"amount\": \"s.amount\",\n",
    "        \"is_high_value\": \"s.is_high_value\",\n",
    "        \"effective_start_date\": \"s.updated_at\",\n",
    "        \"effective_end_date\": \"NULL\",\n",
    "        \"is_current\": \"true\"\n",
    "    })\n",
    "    \n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8087167741300223,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "q1-orders",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
