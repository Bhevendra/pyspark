{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37bb2764-4947-4b0b-8f9e-bd2d6737f409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42adc272-cf0a-49df-a499-c654c3a8222f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "orders_daily_data = [\n",
    "    (\"o1\", \"c1\", \"SHIPPED\", 120.0, \"2025-02-01 10:00:00\", \"2025-02-01 11:00:00\"),\n",
    "    (\"o2\", \"c2\", \"DELIVERED\", 200.0, \"2025-02-01 12:00:00\", \"2025-02-01 12:30:00\"),\n",
    "    (\"o3\", \"c3\", \"PENDING\", -50.0, \"2025-02-01 13:00:00\", \"2025-02-01 13:05:00\"),  # invalid negative\n",
    "    (None, \"c4\", \"SHIPPED\", 80.0, \"2025-02-01 14:00:00\", \"2025-02-01 14:10:00\"), # null order_id\n",
    "    (\"o1\", \"c1\", \"DELIVERED\", 120.0, \"2025-02-01 10:00:00\", \"2025-02-01 15:00:00\"), # newer update\n",
    "    (\"o4\", \"c5\", \"PENDING\", 300.0, \"2025-01-29 09:00:00\", \"2025-02-01 16:00:00\"), # late arriving\n",
    "    (\"o5\", \"c6\", \"CANCELLED\", 150.0, \"2025-02-01 17:00:00\", \"2025-02-01 17:05:00\")\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"order_amount\", DoubleType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True),\n",
    "    StructField(\"last_updated\", StringType(), True),\n",
    "])\n",
    "\n",
    "orders_daily_df = spark.createDataFrame(orders_daily_data, schema=orders_schema)\n",
    "\n",
    "orders_daily_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"/Volumes/pyspark/bronze/raw_ingestion/csv/orders_daily_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c79a4a-ab66-4c37-bbb7-be08b04c839d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "orders_existing_data = [\n",
    "    (\"o1\", \"c1\", \"PENDING\", 120.0, \"2025-02-01 10:00:00\", \"2025-02-01 10:30:00\"),\n",
    "    (\"o2\", \"c2\", \"SHIPPED\", 200.0, \"2025-02-01 12:00:00\", \"2025-02-01 12:15:00\"),\n",
    "    (\"o6\", \"c7\", \"DELIVERED\", 400.0, \"2025-01-30 08:00:00\", \"2025-01-30 09:00:00\")\n",
    "]\n",
    "\n",
    "orders_existing_df = spark.createDataFrame(orders_existing_data, schema=orders_schema)\n",
    "\n",
    "orders_existing_df = (\n",
    "    orders_existing_df\n",
    "        .withColumn(\"order_timestamp\", to_timestamp(col(\"order_timestamp\")))\n",
    "        .withColumn(\"last_updated\", to_timestamp(col(\"last_updated\")))\n",
    ")\n",
    "\n",
    "orders_existing_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"pyspark.silver.t2_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8287da8d-ee68-4ee8-95af-0d0bb135b127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Pair Programming Task 2 â€” Incremental Pipeline + Delta\n",
    "\n",
    "## ðŸŽ¯ Scenario\n",
    "\n",
    "You are building a pipeline for an **Orders system**.\n",
    "\n",
    "You receive daily data from an API.\n",
    "\n",
    "Each file contains:\n",
    "\n",
    "### ðŸ“ `orders_daily.csv`\n",
    "\n",
    "```\n",
    "order_id (string)\n",
    "customer_id (string)\n",
    "order_status (string)\n",
    "order_amount (double)\n",
    "order_timestamp (string)\n",
    "last_updated (string)\n",
    "```\n",
    "\n",
    "### âš ï¸ Important\n",
    "\n",
    "* Multiple updates for same order_id may arrive.\n",
    "* Late-arriving updates are possible.\n",
    "* Some records may already exist in target.\n",
    "* Some orders may change status from:\n",
    "  PENDING â†’ SHIPPED â†’ DELIVERED â†’ CANCELLED\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Target Table (Delta)\n",
    "\n",
    "You have an existing Delta table:\n",
    "\n",
    "`orders_gold`\n",
    "\n",
    "```\n",
    "order_id\n",
    "customer_id\n",
    "order_status\n",
    "order_amount\n",
    "order_timestamp\n",
    "last_updated\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96b37944-ef46-447f-842a-0960e1022297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ðŸ§  Requirements\n",
    "\n",
    "## Step 1 â€” Clean & Standardise\n",
    "\n",
    "* Parse timestamps properly\n",
    "* Remove records with null order_id\n",
    "* Remove obviously invalid records (e.g. negative amount)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20c649f-a743-4ea7-8456-b5623c4fbd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"header\":\"true\",\n",
    "    \"delimiter\":\",\",\n",
    "    \"inferSchema\":\"true\",\n",
    "    \"mode\":\"PERMISSIVE\"\n",
    "}\n",
    "\n",
    "df = ( spark.read\n",
    "      .options(**opts)\n",
    "      .csv(\"/Volumes/pyspark/bronze/raw_ingestion/csv/orders_daily_csv/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379fe208-1cdb-49b9-ac61-d6be4507f4f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp,col\n",
    "\n",
    "cleaned_df = (\n",
    "    df.filter(\"order_id is not null\")\n",
    "    .filter(\"order_amount > 0\")\n",
    "    .withColumn(\"order_timestamp\", to_timestamp(col(\"order_timestamp\")))\n",
    "    .withColumn(\"last_updated\", to_timestamp(col(\"last_updated\")))\n",
    ")\n",
    "\n",
    "cleaned_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5aac6be-4825-468a-83cd-8b656d9128f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 2 â€” Deduplicate Source\n",
    "\n",
    "Keep only the latest record per `order_id` using `last_updated`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e2854f-c6bc-45f8-ad27-458de47b15e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"last_updated\").desc())\n",
    "\n",
    "\n",
    "final_df = (\n",
    "    cleaned_df.withColumn(\"rank\",F.row_number().over(window_spec))\n",
    "    .filter(\"rank == 1\")\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "\n",
    "final_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a5b7e4-5a40-49c8-8775-69a64e1c76bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 3 â€” Incremental Upsert into Delta\n",
    "\n",
    "Logic:\n",
    "\n",
    "* If order_id does NOT exist â†’ INSERT\n",
    "* If exists AND source.last_updated > target.last_updated â†’ UPDATE\n",
    "* Otherwise â†’ ignore\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e361d0c-6028-46ba-9461-6bb982dea202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df = final_df\n",
    "\n",
    "#target_df = spark.read.table(\"pyspark.silver.t2_orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e325be-3886-49ce-ad00-d53ca82bf208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "target_df = DeltaTable.forName(spark, \"pyspark.silver.t2_orders\")\n",
    "\n",
    "\n",
    "(target_df.alias(\"t\")\n",
    " .merge(\n",
    "     source_df.alias(\"s\"),\n",
    "     \"t.order_id = s.order_id\"\n",
    " )\n",
    " .whenMatchedUpdate(\n",
    "     condition=\"s.last_updated > t.last_updated\",\n",
    "     set={\n",
    "         \"t.order_status\": \"s.order_status\",\n",
    "         \"t.order_amount\": \"s.order_amount\",\n",
    "         \"t.last_updated\": \"s.last_updated\",\n",
    "     }\n",
    " )\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f80c4eb-c2c7-445a-acd2-933f81b82307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# combine (make sure schemas align)\n",
    "combined = target_df.unionByName(source_df, allowMissingColumns=True)\n",
    "\n",
    "w = Window.partitionBy(\"order_id\").orderBy(F.col(\"last_updated\").desc_nulls_last())\n",
    "\n",
    "upserted = (combined\n",
    "            .withColumn(\"rn\", F.row_number().over(w))\n",
    "            .filter(\"rn = 1\")\n",
    "            .drop(\"rn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd85f0d-03db-4801-9825-8ba1b794cdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c8b023-18b3-427f-a208-4a6bbfc91a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b2a4475-692e-4dd5-95cc-3f1bd761e65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 4 â€” Late Arriving Data Handling\n",
    "\n",
    "Explain:\n",
    "\n",
    "* What happens if a record from 3 days ago arrives today?\n",
    "* How do you ensure correctness?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c6b942a-268e-4337-a38d-a8f5b89ba3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37d5b64-5b4e-4e41-99f5-bfe03b55a3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e66d7b-ccc8-446a-8186-94a8bcb05af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 5 â€” Partition Strategy\n",
    "\n",
    "Decide:\n",
    "\n",
    "* How would you partition orders_gold?\n",
    "* Why?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8705ad9-412a-457c-a1e8-5ba3a28ba1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a62e63d7-24a0-4fcd-ac96-49ed2fc2b47c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 6 â€” Performance & Scale\n",
    "\n",
    "What if:\n",
    "\n",
    "* 500 million records per day?\n",
    "* 3 years of history?\n",
    "* SLA: 30 minutes?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46638b6c-301d-4209-88d7-4e94918d14ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b2b51f-b8e2-466b-bed1-6f6d31c02519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ðŸš¨ What to Test\n",
    "\n",
    "This task tests:\n",
    "\n",
    "* MERGE correctness\n",
    "* Business key logic\n",
    "* Watermark thinking\n",
    "* Idempotency\n",
    "* Partitioning maturity\n",
    "* Production awareness\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b709f400-dbaa-49b4-b31a-728e4381facb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
