{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5c0687-52ba-4445-9d90-60d9992e350f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dataset Genration for Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6804807-8456-4683-8462-1bfa3bae26ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "transactions_data = [\n",
    "    (\"t1\", \"u1\", \"p1\", \"$120.50\", \"USD\", \"2025-02-01 10:15:00\"),\n",
    "    (\"t2\", \"u2\", \"p2\", \"$200.00\", \"USD\", \"01-02-2025 11:20:00\"),  # inconsistent date\n",
    "    (\"t3\", None, \"p3\", \"$50.00\", \"USD\", \"2025/02/01 12:00:00\"),  # null user\n",
    "    (\"t4\", \"u3\", \"p4\", \"-$30.00\", \"USD\", \"2025-02-01 13:00:00\"), # negative\n",
    "    (\"t5\", \"u1\", \"p5\", \"$75.00\", \"USD\", \"2025-02-01 14:00:00\"),\n",
    "    (\"t5\", \"u1\", \"p5\", \"$75.00\", \"USD\", \"2025-02-01 14:05:00\"),  # duplicate (later timestamp)\n",
    "    (\"t6\", \"u4\", \"p2\", \"$500.00\", \"USD\", \"2025-02-02 09:00:00\"),\n",
    "    (\"t7\", \"u2\", \"p3\", \"$300.00\", \"USD\", \"2025-02-02 10:30:00\"),\n",
    "    (\"t8\", \"u3\", \"p1\", \"$150.00\", \"USD\", \"2025-02-02 11:00:00\"),\n",
    "]\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"amount\", StringType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"transaction_timestamp\", StringType(), True),\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, schema=transactions_schema)\n",
    "\n",
    "transactions_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"/Volumes/pyspark/bronze/raw_ingestion/csv/transactions_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a7019e-0e65-4d57-a68f-3ae990f75dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_data = [\n",
    "    (\"u1\", \"uk\", \"2024-01-01\"),\n",
    "    (\"u2\", \"Us\", \"2024-02-01\"),\n",
    "    (\"u3\", \"IN\", \"2024-03-01\"),\n",
    "    (\"u3\", \"in\", \"2024-03-02\"),  # duplicate user (latest signup)\n",
    "    (\"u4\", None, \"2024-04-01\"),  # null country\n",
    "]\n",
    "\n",
    "users_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"signup_date\", StringType(), True),\n",
    "])\n",
    "\n",
    "users_df = spark.createDataFrame(users_data, schema=users_schema)\n",
    "\n",
    "users_df.write.mode(\"overwrite\").option(\"header\", True).csv(\"/Volumes/pyspark/bronze/raw_ingestion/csv/users_csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2fe0877-e5e3-4562-9559-61983225d3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pair Programming Task â€” End-to-End PySpark Pipeline\n",
    "---\n",
    "## Scenario\n",
    "\n",
    "You are given two datasets:\n",
    "\n",
    "### ðŸ“ `transactions.csv`\n",
    "\n",
    "```\n",
    "transaction_id (string)\n",
    "user_id (string)\n",
    "product_id (string)\n",
    "amount (string)\n",
    "currency (string)\n",
    "transaction_timestamp (string)\n",
    "```\n",
    "\n",
    "Problems:\n",
    "\n",
    "* amount contains currency symbols (e.g. \"$120.50\")\n",
    "* timestamp format inconsistent\n",
    "* duplicates exist\n",
    "* some null user_id rows\n",
    "* some negative amounts\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ `users.csv`\n",
    "\n",
    "```\n",
    "user_id (string)\n",
    "country (string)\n",
    "signup_date (string)\n",
    "```\n",
    "\n",
    "Problems:\n",
    "\n",
    "* duplicate users\n",
    "* country casing inconsistent\n",
    "* null values\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58bad177-f4b9-4934-b489-b148b2a4beca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1 â€” Read Data\n",
    "\n",
    "* Read both datasets\n",
    "* Define schema explicitly\n",
    "* Handle malformed rows safely\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03778a9-a20a-4de0-8d66-e516add087f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_trans = \"transaction_id string, user_id string, product_id string, amount string, currency string, transaction_timestamp string\"\n",
    "\n",
    "schema_user  = \"user_id string, country string, signup_date string\"\n",
    "\n",
    "read_opts = {\n",
    "    \"header\": \"true\",\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "    \"columnNameOfCorruptRecord\": \"_corrupt_record\"\n",
    "}\n",
    "\n",
    "df_trans = (\n",
    "    spark.read\n",
    "      .options(**read_opts)\n",
    "      .schema(schema_trans)\n",
    "      .csv(\"/Volumes/pyspark/bronze/raw_ingestion/csv/transactions_csv/\")\n",
    ")\n",
    "\n",
    "df_user = (\n",
    "    spark.read\n",
    "      .options(**read_opts)\n",
    "      .schema(schema_user)\n",
    "      .csv(\"/Volumes/pyspark/bronze/raw_ingestion/csv/users_csv/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c40309b-fc07-4b10-aeea-87e367bbac47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import _parse_datatype_string as p\n",
    "\n",
    "schema_trans = p(schema_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de031cdb-7eae-4a63-aa80-2f87ab1e3886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_trans.limit(5).display()\n",
    "df_user.limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84c7dce1-2302-4fbf-93d5-6681798030b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2 â€” Clean Data\n",
    "\n",
    "### Transactions:\n",
    "\n",
    "* Remove null user_id\n",
    "* Remove negative amounts\n",
    "* Clean amount â†’ cast to double\n",
    "* Standardise timestamp â†’ proper timestamp type\n",
    "* Deduplicate based on latest transaction_timestamp per transaction_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bc22c3-636e-4bf1-b251-2bf1be73b156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, coalesce, expr\n",
    "\n",
    "df_clean = (df_trans\n",
    "  .withColumn(\"amount\", regexp_replace(col(\"amount\"), r\"\\$\", \"\").cast(\"double\"))\n",
    "  .withColumn(\n",
    "      \"transaction_timestamp\",\n",
    "      coalesce(\n",
    "          expr(\"try_to_timestamp(transaction_timestamp, 'dd-MM-yyyy HH:mm:ss')\"),\n",
    "          expr(\"try_to_timestamp(transaction_timestamp, 'yyyy-MM-dd HH:mm:ss')\")\n",
    "      )\n",
    "  )\n",
    "  .filter(col(\"amount\") > 0)\n",
    "  .filter(col(\"user_id\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf3857e-bf7b-427e-a8f0-398e24bf0bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, coalesce, expr, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "wind_spec = Window.partitionBy(\"transaction_id\").orderBy(col(\"transaction_timestamp\").desc())\n",
    "\n",
    "tran_final = df_clean.withColumn(\"rank\", row_number().over(wind_spec)).filter(col(\"rank\")==1).drop(\"rank\")\n",
    "tran_final.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17acdd6a-997f-49c1-ac84-07866978ecb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Users:\n",
    "\n",
    "* Deduplicate based on latest signup_date\n",
    "* Standardise country to uppercase\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace33ffd-bd5e-42a1-96d2-3d8d6fe54068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "wind_spec_user = Window.partitionBy(\"user_id\").orderBy(col(\"signup_date\").desc())\n",
    "\n",
    "user_final = df_user.withColumn(\"rank\",row_number().over(wind_spec_user)).filter(\"rank==1\").drop(\"rank\")\\\n",
    "    .withColumn(\"country\", upper(col(\"country\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d977b5a-499d-48da-a9ca-f2cc2c0a868d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3 â€” Transform\n",
    "\n",
    "1. Join transactions with users\n",
    "2. Calculate:\n",
    "\n",
    "   * Daily revenue per country\n",
    "3. Return:\n",
    "\n",
    "   * transaction_date\n",
    "   * country\n",
    "   * total_revenue\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3100dac8-a01e-4c09-94b0-fd5528a87272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, sum\n",
    "\n",
    "df = tran_final.join(user_final, [\"user_id\"], \"inner\")\n",
    "df = df.withColumn(\"date\", to_date(col(\"transaction_timestamp\")))\n",
    "\n",
    "df.groupBy(\"country\",\"date\")\\\n",
    "    .agg(sum(\"amount\").alias(\"total_revenue\"))\\\n",
    "    .orderBy(col(\"total_revenue\").desc())\\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1196569e-1358-4efc-bd45-02f1356ff48e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4 â€” Advanced Requirement (Very Likely)\n",
    "\n",
    "For each country and day:\n",
    "\n",
    "* Return top 3 users by revenue\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "transaction_date\n",
    "country\n",
    "user_id\n",
    "total_user_revenue\n",
    "rank\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8685dfd8-3b41-473a-834a-5680bc310477",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771595489961}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_total_rev = df.groupBy(\"user_id\",\"date\",\"country\")\\\n",
    "    .agg(sum(col(\"amount\")).alias(\"total_user_revenue\"))\n",
    "\n",
    "user_total_rev.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847c8e51-79bd-4cd3-8509-1d59d5f97b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"country\",\"date\").orderBy(col(\"total_user_revenue\").desc())\n",
    "user_total_rev = (\n",
    "    user_total_rev.withColumn(\n",
    "        \"rank\", row_number().over(window_spec)\n",
    "    )\n",
    "    .filter(col(\"rank\")<= 3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d671ba6-9ea8-45fa-bc45-dd18f8594563",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771595551628}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_total_rev.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3740d7-b505-462d-9da3-232d92405121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Step 5 â€” Performance Discussion\n",
    "\n",
    "After coding they will ask:\n",
    "\n",
    "* What causes shuffle here?\n",
    "* Where would you broadcast?\n",
    "* How would you optimise for 1 billion rows?\n",
    "* How would you partition when writing?\n",
    "* How would you productionise this?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97f4295-387d-4286-ba9d-dcea696c1322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
